import os
import time
import requests
import google.generativeai as genai
from flask import Flask, request

# CONFIGURA√á√ïES
Z_API_ID = "3EC3280430DD02449072061BA788E473"
Z_API_TOKEN = "34E8E958D060C21D55F5A3D8"
CLIENT_TOKEN = "Ff1119996b44848dbaf394270f9933163S"
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")

app = Flask(__name__)

if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)

# Mem√≥ria vol√°til (RAM)
chat_sessions = {}

# PROMPT COM L√ìGICA DE MEM√ìRIA
PROMPT_SISTEMA = """
Voc√™ √© o Pedro Lima, consultor da Microlins.
SEU OBJETIVO: Coletar 4 informa√ß√µes (Cidade, √Årea atual, Prazo e Capital).

REGRAS DE MEM√ìRIA:
1. Analise o hist√≥rico abaixo antes de perguntar.
2. Se o lead j√° respondeu uma pergunta, PASSE PARA A PR√ìXIMA.
3. N√£o seja repetitivo. Se ele disse "Sou do Rio", n√£o pergunte a cidade novamente.

ORDEM DAS PERGUNTAS:
1¬∫ Cidade? -> 2¬∫ √Årea de atua√ß√£o? -> 3¬∫ Prazo (3 meses ou +)? -> 4¬∫ Capital dispon√≠vel?
"""

def gerar_resposta_ia(phone, mensagem_usuario):
    # Mudan√ßa para modelos com maior probabilidade de cota dispon√≠vel
    modelos_candidatos = [
        "gemini-1.5-flash", # Modelo com maior cota gratuita (1500 req/dia)
        "gemini-pro",       # Modelo est√°vel cl√°ssico
        "gemini-1.0-pro"    # √öltimo recurso
    ]

    if phone not in chat_sessions:
        chat_sessions[phone] = []

    # Mant√©m apenas as √∫ltimas 10 mensagens para n√£o gastar tokens
    historico_curto = chat_sessions[phone][-10:]

    for nome_modelo in modelos_candidatos:
        try:
            print(f"üîÑ Tentando {nome_modelo}...", flush=True)
            model = genai.GenerativeModel(nome_modelo)
            
            # Formata o hist√≥rico para a IA entender o contexto
            contexto_com_historico = f"{PROMPT_SISTEMA}\n\nHist√≥rico atual: {historico_curto}\n\nLead disse agora: {mensagem_usuario}"
            
            response = model.generate_content(contexto_com_historico)
            resposta_texto = response.text

            # Salva no hist√≥rico a troca de mensagens
            chat_sessions[phone].append(f"Lead: {mensagem_usuario}")
            chat_sessions[phone].append(f"Pedro: {resposta_texto}")
            
            return resposta_texto

        except Exception as e:
            print(f"‚ùå Erro no {nome_modelo}: {e}", flush=True)
            if "429" in str(e):
                continue # Tenta o pr√≥ximo modelo
            continue

    return "Oi! Recebi sua mensagem, mas estou processando algumas informa√ß√µes aqui. Pode me dar 1 minutinho e j√° te respondo?"

@app.route("/webhook", methods=["POST"])
def webhook():
    data = request.json or {}
    if data.get("fromMe"): return "ok", 200

    msg = data.get("text", {}).get("message")
    phone = data.get("phone")

    if msg and phone:
        print(f"üì© Lead: {msg}", flush=True)
        resp = gerar_resposta_ia(phone, msg)
        
        requests.post(
            f"https://api.z-api.io/instances/{Z_API_ID}/token/{Z_API_TOKEN}/send-text",
            json={"phone": phone, "message": resp}, 
            headers={"Client-Token": CLIENT_TOKEN, "Content-Type": "application/json"}
        )
            
    return "ok", 200

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 5000))
    app.run(host='0.0.0.0', port=port)
